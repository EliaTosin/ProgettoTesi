{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%html\n",
    "<style>\n",
    "table {float:left}\n",
    "</style>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data analysis:\n",
    "\n",
    "### Train set\n",
    "\n",
    "| Label name | | Number of images |\n",
    "| - | | - | \n",
    "| hyundai | | 302 | \n",
    "| lexus | | 301 | \n",
    "| mazda | | 317 | \n",
    "| mercedes | | 342 | \n",
    "| opel | | 301 | \n",
    "| skoda | | 314 | \n",
    "| toyota | | 306 | \n",
    "| volkswagen | | 330 | "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test set\n",
    "| Label name | | Number of images |\n",
    "| - | | - | \n",
    "| hyundai | | 50 | \n",
    "| lexus | | 50 | \n",
    "| mazda | | 50 | \n",
    "| mercedes | | 50 | \n",
    "| opel | | 50 | \n",
    "| skoda | | 50 | \n",
    "| toyota | | 50 | \n",
    "| volkswagen | | 50 | "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor, Resize, ToPILImage\n",
    "from torchvision.transforms.functional import to_grayscale, to_tensor, rotate\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from torchvision.io import read_image\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torchvision.utils import make_grid\n",
    "import seaborn as sn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Useful functions to retrive the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a annotations_file.csv (index -> img (name), label (int))\n",
    "\n",
    "def getLabelList(path):\n",
    "    only_dirs = [ name for name in os.listdir(path) if \n",
    "                 os.path.isdir(os.path.join(path, name)) ]\n",
    "\n",
    "    ret = {}\n",
    "    index = 0\n",
    "    \n",
    "    for d in only_dirs:\n",
    "        new_path = path + d\n",
    "        label = only_dirs.index(d)\n",
    "        # print(len(os.listdir(new_path)))\n",
    "        for img in [ name for name in os.listdir(new_path) ]:\n",
    "            ret[index] = [img, label]\n",
    "            index += 1\n",
    "    \n",
    "    return pd.Series(ret)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the pool of images (with a series that contains all the images, more deeply: index -> abs path, label (int), class (string))\n",
    "\n",
    "def getImgPool(path):\n",
    "    only_dirs = [ name for name in os.listdir(path) if \n",
    "                 os.path.isdir(os.path.join(path, name)) ]\n",
    "\n",
    "    ret = {}\n",
    "    index = 0\n",
    "    \n",
    "    for d in only_dirs:\n",
    "        new_path = path + d\n",
    "        label = only_dirs.index(d)\n",
    "        # cnt = 0\n",
    "        for img in [ name for name in os.listdir(new_path) ]:\n",
    "            abs_path = new_path + '/' + img\n",
    "            ret[index] = [abs_path, label, d]\n",
    "            index += 1\n",
    "            # cnt += 1\n",
    "        # print(f\"Label: {d} - Number: {cnt}\")\n",
    "    \n",
    "    return pd.Series(ret)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the custom dataset (structure that holds the data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wrong_images_path = []\n",
    "import traceback\n",
    "\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, path_labels, transform=None, target_transform=None):\n",
    "        self.img_labels = getLabelList(path_labels)\n",
    "        self.images = getImgPool(path_labels)\n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.img_labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # try: \n",
    "        image = read_image(self.images.loc[idx][0])\n",
    "        # except Exception:\n",
    "        #     print('Found error at {} - {}'.format(self.images.loc[img_name][0], self.img_labels.iloc[idx][1]))\n",
    "        #     wrong_images_path.append(self.images.loc[img_name][0])\n",
    "        #     return torch.zeros((600, 600, 1)), -1\n",
    "        # image = read_image(self.images.loc[img_name][0])\n",
    "        if image.shape[0] >= 3: #apply grayscale to colored img\n",
    "            image = to_tensor(to_grayscale(ToPILImage()(image), num_output_channels=1))\n",
    "        else: #we have a greyscaled image, move it into tensor \n",
    "            image = to_tensor(ToPILImage()(image))\n",
    "            \n",
    "        image = Resize((600, 600))(image)\n",
    "        #data augmentation\n",
    "        prob = torch.randint(100, size=(1,)).item()\n",
    "        if prob > 20: \n",
    "            angle = torch.randint(60, size=(1,)).item() - 30\n",
    "            image = rotate(image, angle)        \n",
    "        \n",
    "        image = image.permute(1,2,0) #made to be shown on matplotlib\n",
    "        label = self.img_labels.loc[idx][1]\n",
    "        \n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the 2 dataset and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "train_path = './data/car_brand_logos/Train/'\n",
    "test_path = './data/car_brand_logos/Test/'\n",
    "\n",
    "# train_data = CustomImageDataset(path_labels=train_path, transform=ToTensor(), num_of_channels=3)\n",
    "# test_data = CustomImageDataset(path_labels=test_path, transform=ToTensor(), num_of_channels=3)\n",
    "\n",
    "train_data = CustomImageDataset(path_labels=train_path, transform=ToTensor())\n",
    "\n",
    "test_data = CustomImageDataset(path_labels=test_path, transform=ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(train_data.__len__()):\n",
    "#     img, label = train_data[i]\n",
    "\n",
    "# for i in range(test_data.__len__()):\n",
    "#     img, label = test_data[i]\n",
    "    \n",
    "    \n",
    "# wrong_images_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#testing dataset if images are loaded\n",
    "\n",
    "only_dirs = [ name for name in os.listdir(train_path) if \n",
    "                 os.path.isdir(os.path.join(train_path, name)) ]\n",
    "\n",
    "diz = {}\n",
    "nocl=0 #num of classes\n",
    "for d in only_dirs:\n",
    "    diz[nocl] = d\n",
    "    nocl+=1\n",
    "\n",
    "figure = plt.figure(figsize=(8, 8))\n",
    "cols, rows = 3, 3\n",
    "for i in range(1, cols * rows + 1):\n",
    "    sample_idx = torch.randint(len(train_data), size=(1,)).item() \n",
    "    img, label = train_data[sample_idx]\n",
    "    figure.add_subplot(rows, cols, i)\n",
    "    plt.title(diz[label])\n",
    "    plt.axis(\"off\")\n",
    "    plt.imshow(img.squeeze(), cmap='gray')\n",
    "    # plt.imshow(img.squeeze())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# img, label = train_data[300]\n",
    "\n",
    "# print(img.shape)\n",
    "\n",
    "# plt.imshow(img.squeeze(), cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### creating dataloader (who iterate on data, create batches and shuffles it)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DataLoader(train_data, batch_size=8, shuffle=True)\n",
    "test_dl = DataLoader(test_data, batch_size=8, shuffle=True)\n",
    "\n",
    "#dataloader test (ask a batch or a sample)\n",
    "images, labels = next(iter(train_dl)) #picking a batch\n",
    "print(f\"Feature batch shape: {images.size()}\")\n",
    "print(f\"Labels batch shape: {labels.size()}\")\n",
    "img = images[0].squeeze() #picking the first img and label of the batch\n",
    "label = labels[0]\n",
    "plt.imshow(img, cmap='gray')\n",
    "plt.show()\n",
    "print(f\"Label: {diz[label.item()]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.rename('./data/car_brand_logos/Train/mercedes/r6.JPG', './data/car_brand_logos/Train/mercedes/r6.jpg')\n",
    "# image = read_image('./data/car_brand_logos/Train/volkswagen/vosvos-volkswagen-logo-patches-arma-pec-kot-yamasi__1548797389090801.jpg')\n",
    "# image = image.permute(1,2,0)\n",
    "# plt.imshow(image, cmap='gray')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        #conv1: 1 input image channel (image channel, 1 gray, 3 rgb), 6 output channels (depth [K]), 5x5 square convolution kernel, DEFAULT: stride = 1,1, padding = 0\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.conv2 = nn.Conv2d(6, 10, 5)        \n",
    "        self.fc_layer1 = nn.Linear(10*147*147, 120)\n",
    "        \n",
    "        self.fc_layer2 = nn.Linear(120, 84)\n",
    "        self.fc_layer3 = nn.Linear(84, nocl) # nocl is the num of classes\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Max pooling over a (2, 2) window\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
    "        # If the size is a square, you can specify with a single number\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        x = torch.flatten(x, 1) # flatten all dimensions except the batch dimension\n",
    "        x = F.relu(self.fc_layer1(x))\n",
    "        x = F.relu(self.fc_layer2(x))\n",
    "        x = self.fc_layer3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Net()\n",
    "# net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test\n",
    "params = list(net.parameters())\n",
    "print(len(params))\n",
    "print(params[0].size())  # conv1's .weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# use gradient descent\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorboard setup\n",
    "\n",
    "start tensorboard: `tensorboard --logdir=Users\\eliad\\Tesi\\ProgettoTesi\\runs\\car_brands_1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writer = SummaryWriter('runs/car_brands_2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get some random training images\n",
    "dataiter = iter(train_dl)\n",
    "images, labels = dataiter.next()\n",
    "\n",
    "#visualizing and adding images into tb\n",
    "for idx in range(4):\n",
    "    plt.imshow(images[idx], cmap='gray')\n",
    "    plt.show()\n",
    "    img = images[idx]\n",
    "    img = img.permute(2,0,1)\n",
    "    writer.add_image(f\"car_brands_images_{idx+1}\", img)\n",
    "    \n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding the graph of the network into Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = images.permute(0, 3, 1, 2) #move to tensor mode\n",
    "\n",
    "writer.add_graph(net, images)\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding the projector "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_dl = DataLoader(train_data, batch_size=100, shuffle=True)\n",
    "images, labels = next(iter(tmp_dl))\n",
    "\n",
    "# get the class labels for each image\n",
    "class_labels = [diz[lab.item()] for lab in labels]\n",
    "\n",
    "\n",
    "images = images.permute(0,3,1,2) #tensor mode\n",
    "images = Resize((28, 28))(images)\n",
    "\n",
    "features = images.view(-1, 28 * 28)\n",
    "writer.add_embedding(features,\n",
    "                    metadata=class_labels,\n",
    "                    label_img=images)\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion matrix during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createConfusionMatrix(loader):\n",
    "    y_pred = [] # save predction\n",
    "    y_true = [] # save ground truth\n",
    "\n",
    "    # iterate over data\n",
    "    for inputs, labels in loader:\n",
    "        inputs = inputs.permute(0, 3, 1, 2)\n",
    "        output = net(inputs)  # Feed Network\n",
    "\n",
    "        output = (torch.max(torch.exp(output), 1)[1]).data.cpu().numpy()\n",
    "        y_pred.extend(output)  # save prediction\n",
    "\n",
    "        labels = labels.data.cpu().numpy()\n",
    "        y_true.extend(labels)  # save ground truth\n",
    "\n",
    "    # constant for classes\n",
    "    classes = diz.values()\n",
    "\n",
    "    # Build confusion matrix\n",
    "    cf_matrix = confusion_matrix(y_true, y_pred)\n",
    "    df_cm = pd.DataFrame(cf_matrix/np.sum(cf_matrix) * 10, index=[i for i in classes],\n",
    "                         columns=[i for i in classes])\n",
    "    plt.figure(figsize=(10, 7), dpi=150)    \n",
    "    return sn.heatmap(df_cm, annot=True).get_figure()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(3):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_dl, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "        # inputs, labels = data[0].to(device), data[1].to(device)\n",
    "        inputs = inputs.permute(0, 3, 1, 2) # to get a shape of [64, 1, 600, 600]\n",
    "\n",
    "        # inputs = inputs.view(64, -1) #1Â° param specifico io, altri calcola lui (-1)\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 45 == 44:    # print every 360 images (8 images per batch, 45 loops)\n",
    "            print(f'[{epoch + 1}, {i+1}/{len(train_dl)}] loss: {running_loss:.3f}')\n",
    "\n",
    "            # ...log the running loss\n",
    "            writer.add_scalar('training loss',\n",
    "                        running_loss / 1000,\n",
    "                        epoch * len(train_dl) + i)\n",
    "            running_loss = 0.0\n",
    "\n",
    "    #end of an epoch, calculate loss on test set (since it's small and there's no need to split)\n",
    "    test_loss = 0\n",
    "    for _, data in enumerate(test_dl):\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.permute(0, 3, 1, 2)\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        test_loss += loss.item()\n",
    "    print(f\"[{epoch + 1}] test loss: {test_loss:.3f}\")\n",
    "    # confusion matrix (added from extra step)\n",
    "    writer.add_figure(\"Confusion matrix\", createConfusionMatrix(test_dl), epoch)\n",
    "\n",
    "print('Finished Training')\n",
    "\n",
    "#saving the trained model\n",
    "PATH = './trained/auto_brand2.pth'\n",
    "torch.save(net.state_dict(), PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the network on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded = Net()\n",
    "loaded.load_state_dict(torch.load(\"./trained/auto_brand.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images, labels = next(iter(test_dl)) #picking a batch\n",
    "\n",
    "for i in range(1):\n",
    "    rnd_i = torch.randint(len(images), size=(1,)).item() \n",
    "    img = images[rnd_i].squeeze()\n",
    "    label = labels[rnd_i]\n",
    "    plt.imshow(img, cmap='gray')\n",
    "    plt.suptitle('True: {}'.format(diz[label.item()]))\n",
    "    \n",
    "    #predicting the class of the image\n",
    "    imgs = images.permute(0, 3, 1, 2)\n",
    "    preds = net(imgs)\n",
    "    _, best_pred = torch.max(preds.data, 1)\n",
    "    plt.title('Predicted: {}'.format(diz[best_pred[rnd_i].item()]))\n",
    "    # print(f\"Numero di classi: {nocl}\")\n",
    "    # print(f\"Dimensioni delle predizioni del batch da 64 elementi: {preds.size()}\")\n",
    "    # print(f\"Predizioni dell'elemento scelto: {preds[rnd_i]}\")\n",
    "    # print(diz.values())\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare to count predictions for each class\n",
    "correct_pred = {classname: 0 for classname in diz}\n",
    "total_pred = {classname: 0 for classname in diz}\n",
    "\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "# again no gradients needed\n",
    "with torch.no_grad():\n",
    "    for data in test_dl:\n",
    "        images, labels = data\n",
    "        images = images.permute(0, 3, 1, 2)\n",
    "        outputs = net(images)\n",
    "        _, predictions = torch.max(outputs, 1)\n",
    "        # collect the correct predictions for each class\n",
    "        for label, prediction in zip(labels, predictions):\n",
    "            if label == prediction:\n",
    "                correct_pred[label.item()] += 1\n",
    "            total_pred[label.item()] += 1\n",
    "            y_true.append(label)\n",
    "            y_pred.append(prediction)\n",
    "\n",
    "# print accuracy for each class\n",
    "for classname, correct_count in correct_pred.items():\n",
    "    accuracy = 100 * float(correct_count) / total_pred[classname]\n",
    "    print(f'Accuracy for class: {diz[classname]} is {accuracy:.1f} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stampa report classificazioni\n",
    "print(classification_report(y_true, y_pred, target_names=diz.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# writer.add_figure(\"Final matrix\", createConfusionMatrix(test_dl), 0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
